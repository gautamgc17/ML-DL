# -*- coding: utf-8 -*-
"""Sentiment-Analysis-using-NeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfFXSXT6q9-VieWHUK14qCJZeTuMb7N6

## **Classifying Movie Reviews**
### Binary Classification on IMDB DataSet
### Inputs : 50,000 Reviews
### Output : Positive and Negative
"""

from keras.datasets import imdb

"""### Preparing IMDB Dataset"""

((x_train, y_train), (x_test , y_test)) = imdb.load_data(num_words = 10000)

print(len(x_train))
print(len(x_test))

word_idx = imdb.get_word_index()    # dictionary of words mapped to their corresponding indexes in IMDB dataset
print(word_idx.items())

idx_word = {index:word for word, index in word_idx.items()}     # Reverse Mapping using dictionary comprehension
print(idx_word.items())

# when vocabulary is created, it also adds 3 words on top of it - <pad> : pad word , <s> : start of word , <unk> : unknown word
actual_review = " ".join([idx_word.get(idx-3  , " ") for idx in x_train[0]])
print(actual_review)

"""#### Next Step - Vectorize the Data
#### Vocab Size - 10,000 we will make sure every sentence is represented by a vector of len 10,000 [0000011010111010011100]
"""

import numpy as np

def vectorize_sentences(sentences , dim = 10000):
  outputs = np.zeros((len(sentences) , dim))

  for i, idx in enumerate(sentences):
    outputs[i, idx] = 1

  return outputs

X_train = vectorize_sentences(x_train)
X_test = vectorize_sentences(x_test)

print(X_train.shape , X_test.shape)

X_train

Y_train = np.array(y_train).astype('float32')
Y_test = np.array(y_test).astype('float32')

print(Y_train.shape , Y_test.shape)

"""### Building and Compiling our Neural Network
#### **Define the Model Architecture**
#### Use Fully Connected/Dense layers with Relu Activation
#### 2 Hidden Layers with 16 units each
#### 1 Output Layer with 1 unit (Sigmoid Activation)
"""

from keras.models import Sequential
from keras.layers import Dense

# Define the model
model = Sequential()
model.add(Dense(16, activation = 'relu', input_shape = (10000, )))
model.add(Dense(16, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# Compile the model
model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])

model.summary()

"""### Evaluation and Early Stopping
#### Training and Validation
"""

x_val = X_train[:5000]
x_train_new = X_train[5000:]

y_val = Y_train[:5000]
y_train_new = Y_train[5000:]

hist = model.fit(x_train_new, y_train_new, batch_size=512, epochs=20, validation_data=(x_val, y_val))

"""### Visualize the Data"""

import matplotlib.pyplot as plt

## contains dictionary with accuracy and loss as key values
h = hist.history
h.keys()

plt.plot(h['val_loss'] , label = 'Validation Loss')
plt.plot(h['loss'] , label = 'Training Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

## early stopping point - 4 epochs
plt.plot(h['val_accuracy'] , label = 'Validation Accuracy')
plt.plot(h['accuracy'] , label = 'Training Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.show()

hist = model.fit(x_train_new, y_train_new, batch_size=512, epochs=4, validation_data=(x_val, y_val))
h = hist.history

plt.plot(h['val_loss'] , label = 'Validation Loss')
plt.plot(h['loss'] , label = 'Training Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(h['val_accuracy'] , label = 'Validation Accuracy')
plt.plot(h['accuracy'] , label = 'Training Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

## Returns the loss value & metrics values
model.evaluate(X_train , Y_train)

eval = model.evaluate(X_test , Y_test)
print("Test Accuracy:" , eval[1])

## generate output probabilities for each class
probas = model.predict(X_test)
probas

labels = (probas >  0.5).astype(np.int)
print(labels.shape)
print(labels)

Y_test = Y_test.reshape((-1, 1))
Y_test.shape

acc = np.sum(labels == Y_test)/Y_test.shape[0]
print("Test Accuracy:" , acc*100)