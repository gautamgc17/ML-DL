{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "print(len(envs.registry.all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03139016,  0.02794889, -0.04401998,  0.02013752])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "random_action = env.action_space.sample()\n",
    "print(random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode: 0/20 and High Score is: 22\n",
      "[-0.00235619 -0.23200582  0.21244283  0.97276862] 1.0 True {}\n",
      "Game Episode: 1/20 and High Score is: 14\n",
      "[ 0.13257581  0.64573249 -0.21309266 -1.32279238] 1.0 True {}\n",
      "Game Episode: 2/20 and High Score is: 17\n",
      "[ 0.15239779  0.45460986 -0.22693169 -1.0282224 ] 1.0 True {}\n",
      "Game Episode: 3/20 and High Score is: 25\n",
      "[-0.07834063 -0.35494073  0.23032277  0.92595248] 1.0 True {}\n",
      "Game Episode: 4/20 and High Score is: 13\n",
      "[ 0.10894059  0.38012041 -0.21387581 -1.00471022] 1.0 True {}\n",
      "Game Episode: 5/20 and High Score is: 23\n",
      "[ 0.17231007  0.45602003 -0.2239744  -1.1073463 ] 1.0 True {}\n",
      "Game Episode: 6/20 and High Score is: 20\n",
      "[ 0.18342353  1.38206447 -0.21165518 -2.14023524] 1.0 True {}\n",
      "Game Episode: 7/20 and High Score is: 26\n",
      "[-0.20001621 -0.58749505  0.2228952   1.19982945] 1.0 True {}\n",
      "Game Episode: 8/20 and High Score is: 11\n",
      "[ 0.08275216  0.36205401 -0.22159364 -0.93886645] 1.0 True {}\n",
      "Game Episode: 9/20 and High Score is: 21\n",
      "[-0.24776151 -1.58236232  0.25215626  2.49324058] 1.0 True {}\n",
      "Game Episode: 10/20 and High Score is: 10\n",
      "[-0.06759074 -0.60650103  0.22962942  1.23304596] 1.0 True {}\n",
      "Game Episode: 11/20 and High Score is: 10\n",
      "[-0.10850269 -0.56191578  0.23339822  1.19536962] 1.0 True {}\n",
      "Game Episode: 12/20 and High Score is: 12\n",
      "[ 0.0773377   0.60289218 -0.21249142 -1.2411171 ] 1.0 True {}\n",
      "Game Episode: 13/20 and High Score is: 30\n",
      "[-0.07878974 -0.15995435  0.21522049  0.61022444] 1.0 True {}\n",
      "Game Episode: 14/20 and High Score is: 20\n",
      "[ 0.03891479  0.18446424 -0.21364962 -0.82291237] 1.0 True {}\n",
      "Game Episode: 15/20 and High Score is: 19\n",
      "[ 0.20893986  1.59874624 -0.25091048 -2.62528812] 1.0 True {}\n",
      "Game Episode: 16/20 and High Score is: 12\n",
      "[-0.11695757 -0.25726746  0.21709435  0.75395154] 1.0 True {}\n",
      "Game Episode: 17/20 and High Score is: 19\n",
      "[-0.09248548  0.00891415  0.21255901  0.57947038] 1.0 True {}\n",
      "Game Episode: 18/20 and High Score is: 10\n",
      "[-0.11939513 -0.60132467  0.22016408  1.15258797] 1.0 True {}\n",
      "Game Episode: 19/20 and High Score is: 43\n",
      "[ 0.26870178  1.13421502 -0.23572458 -1.66901561] 1.0 True {}\n",
      "All 20 episodes over!\n"
     ]
    }
   ],
   "source": [
    "for game_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for timesteps in range(50):\n",
    "        env.render()\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, other_info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode: {}/{} and High Score is: {}\".format(game_episode, 20, timesteps))\n",
    "            break\n",
    "    print(observation, reward, done, other_info)\n",
    "    \n",
    "env.close()\n",
    "print(\"All 20 episodes over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Design and Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, deque_size = 2000, gamma = 0.95, learning_rate = 0.001):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = deque_size)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.model = self._create_model()\n",
    "        \n",
    "    def _create_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        \n",
    "        model.compile(loss = 'mse', optimizer = Adam(learning_rate = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))   # Remember past Experiences\n",
    "        \n",
    "    def action(self, state):\n",
    "        \n",
    "        # Sampling according to the Epsilon Greedy Method\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)   # Take a random action\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def train(self, batch_size = 32):\n",
    "        \n",
    "        # Training using replay buffer technique\n",
    "        minibatch = random.sample(self.memory , batch_size)\n",
    "        for experience in minibatch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            \n",
    "            if not done:\n",
    "                # game is not yet over, bellman equation to approx the target_value of reward\n",
    "                expected_discount_return = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                expected_discount_return = reward\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = expected_discount_return\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs = 1, verbose = 0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN Agent (Deep Q-Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "batch_size = 32\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print(state_size , action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size , action_size)\n",
    "done = False\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1, state_size])\n",
    "    \n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done , other_info = env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state , [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{}, High Score: {}, Exploration Rate: {:.2f}\".format(episode, n_episodes, t, agent.epsilon))\n",
    "            break\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        \n",
    "    print(next_state, reward, done, other_info)\n",
    "    \n",
    "        \n",
    "print('Deep Q-Learner Model Trained - All episodes over!')   \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing game with learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/20 High Score :181\n",
      "Game Episode :1/20 High Score :199\n",
      "Game Episode :2/20 High Score :199\n",
      "Game Episode :3/20 High Score :199\n",
      "Game Episode :4/20 High Score :199\n",
      "Game Episode :5/20 High Score :193\n",
      "Game Episode :6/20 High Score :199\n",
      "Game Episode :7/20 High Score :199\n",
      "Game Episode :8/20 High Score :199\n",
      "Game Episode :9/20 High Score :191\n",
      "Game Episode :10/20 High Score :199\n",
      "Game Episode :11/20 High Score :199\n",
      "Game Episode :12/20 High Score :194\n",
      "Game Episode :13/20 High Score :199\n",
      "Game Episode :14/20 High Score :199\n",
      "Game Episode :15/20 High Score :199\n",
      "Game Episode :16/20 High Score :199\n",
      "Game Episode :17/20 High Score :199\n",
      "Game Episode :18/20 High Score :199\n",
      "Game Episode :19/20 High Score :199\n",
      "All 20 episode over!\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(20):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    \n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = np.argmax(agent.model.predict(state)[0])\n",
    "        next_state, reward, done, other_info = env.step(action)\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{} High Score :{}\".format(e,20,t))\n",
    "            break\n",
    "            \n",
    "env.close()\n",
    "print(\"All 20 episodes over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
